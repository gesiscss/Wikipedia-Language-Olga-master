{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot. Find names from Name_entity_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package ner2.de to C:\\Anaconda\\Lib\\site-\n",
      "[polyglot_data]     packages\\polyglot\\data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====Download this before polyglot usage=====\n",
    "from polyglot.downloader import downloader\n",
    "downloader.supported_tasks(lang=\"de\")\n",
    "downloader.download(\"embeddings2.de\")\n",
    "downloader.download(\"pos2.de\")\n",
    "downloader.download(\"ner2.de\")\n",
    "#========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "import json\n",
    "\n",
    "def identify_name(raw_text):\n",
    "    list_of_personal_names=[]\n",
    "    text = Text(raw_text)\n",
    "    text._BaseBlob__lang=polyglot.detect.base.Language.from_code(\"de\") #we assign to the text German language!!\n",
    "    try:\n",
    "        if text.language.code!=\"de\":\n",
    "            print text.language.code\n",
    "    except Exception, e:\n",
    "        print 'failed in language detection'\n",
    "        print str(e)\n",
    "        return list_of_personal_names,True\n",
    "    try:\n",
    "        for t in text.entities:\n",
    "            if (t.tag==\"I-PER\"):\n",
    "                if (len(t[0])!=1)&(t[0].istitle()):\n",
    "                    list_of_personal_names.append(t[0] if len(t)==1 else \" \".join(i for i in t))# if i.isalpha()))   \n",
    "        return list_of_personal_names,False\n",
    "    except Exception, e:\n",
    "        print 'failed in the entities'\n",
    "        print str(e)\n",
    "        return list_of_personal_names,True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed in the entities\n",
      "'ascii' codec can't encode character u'\\xd6' in position 0: ordinal not in range(128)\n",
      "error at text of: Flugbegleiter\n"
     ]
    }
   ],
   "source": [
    "#====progressbar   \n",
    "#from progress.bar import Bar\n",
    "#bar = Bar('Processing', max=20)\n",
    "#bar.start()\n",
    "#============\n",
    "\n",
    "\n",
    "with open('de/wiki/all_data_from pages.json', 'r') as in_f:\n",
    "    Dump=json.load(in_f)\n",
    "names={}\n",
    "error_list=[]\n",
    "for i in Dump:\n",
    "    #bar.next()\n",
    "    ne,error=identify_name(Dump[i][\"text\"].encode(\"utf-8\").decode(\"utf-8\").replace(\"\\n\",\", \"))\n",
    "    if error:\n",
    "        print \"error at text of:\",i\n",
    "        error_list.append(i)\n",
    "    else:\n",
    "        name_list=list(set(ne))#delete duplicates\n",
    "        name_list=filter(lambda k: i not in k, name_list)#clean from name of article\n",
    "        name_list=[p for p in name_list if not((\" e .\" in p)|(\" e.\" in p))]#clean from e.V.\n",
    "        name_list=[p for p in name_list if not(p in [\"Erw .\",\"Hon .\",\"Sta .\",\"Ed .\",\"Ma ?\",\"Ass .\",\"Kyu -\",\"Éd .\".decode(\"utf-8\"),\"Aja -\"])] \n",
    "        \n",
    "        #replace non aplchabetical values at the end of full name\n",
    "        new_p=[]\n",
    "        old_p=[]\n",
    "        for p in name_list:\n",
    "            if not(p[-1:].isalpha()):\n",
    "                new_p_=p[:-2]\n",
    "                if new_p_[-1:]==\".\":\n",
    "                    new_p_=p[:-4]\n",
    "                #print p, new_p\n",
    "                new_p.append(new_p_)\n",
    "                old_p.append(p)\n",
    "        name_list=list(set(name_list)-set(old_p))\n",
    "        name_list.extend(new_p)\n",
    "        name_list=list(set(name_list)) \n",
    "        name_list=[p for p in name_list if not((\"Ed\"==p)|(\"Verl\"== p))]#clean Ed and Verl\n",
    "        name_list=[p for p in name_list if not((\"Mr\"==p)|(\"Co\"==p)|(\"Ch\"==p)|(\"Chr\"==p)|(\"Sc\"==p)|(\"St\"==p))]#clean Mr Co Ch Chr Sc St\n",
    "        names[i]=name_list\n",
    "#bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Maria Weckesser', u'Saftschubse', u'Strahlenexposition', u'Passagiersitzplatz', u'Nelly Diener', u'Heather Poole', u'Guinness', u'Flugg\\xe4sten', u'Kathrin Leineweber', u'Ellen Church', u'Ingo Matuschek', u'Annette Lies', u'Ron Akana', u'Andrea Brandl', u'Annette', u'Gray', u'Piper']\n"
     ]
    }
   ],
   "source": [
    "#get from error data# scip not readable char\n",
    "#error_list[0]#\"Flugbegleiter\"\n",
    "ne,error=identify_name(Dump[\"Flugbegleiter\"][\"text\"][:14036].replace(\"\\n\",\", \")+\" \"+Dump[\"Flugbegleiter\"][\"text\"][14050:].replace(\"\\n\",\", \"))\n",
    "if error:\n",
    "    print \"error at text of:\",i\n",
    "    error_list.append(i)\n",
    "else:\n",
    "    ne=list(set(ne))\n",
    "    ne=filter(lambda k: \"Flugbegleiter\" not in k, ne)#clean from name of article\n",
    "    ne=[p.replace(\" .\",\"\") for p in ne]#clean from \" .\"\n",
    "    print ne\n",
    "    names[\"Flugbegleiter\"]=ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n"
     ]
    }
   ],
   "source": [
    "print len(names)#,len(Dump)\n",
    "with open('de/wiki/all_name_entity_person_polyglot.json', 'w') as out:\n",
    "    json.dump(names, out, indent=4, sort_keys=True,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter first names and lastnames when repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Add\n"
     ]
    }
   ],
   "source": [
    "#if first name of full name\n",
    "#if last name of full name\n",
    "#if full name ends or beginsw ith this name\n",
    "###in more detail:\n",
    "###first two names, check that there are two names and check at the begining and end of full name\n",
    "###Ex: Matthäus Merian => Matthäus Merian der Ältere\n",
    "\n",
    "#if second/third name in full name\n",
    "\n",
    "#add Marcel Marceau, del all with Marceau\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "with open('de/wiki/all_name_entity_person_polyglot.json', 'r') as in_f:\n",
    "    names=json.load(in_f,encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "to_del=defaultdict(list)\n",
    "to_add=defaultdict(list)\n",
    "for prof in names:\n",
    "    #print prof\n",
    "    for p in names[prof]:\n",
    "        for p2 in names[prof]:\n",
    "            if (p not in to_del[prof]):#|(p2 not in to_del[prof]):\n",
    "                if p!=p2:\n",
    "                    if p in p2:\n",
    "                        if p+\"s\"==p2:\n",
    "                            to_del[prof].append(p2) \n",
    "                            #print p,\"=>\",p2\n",
    "                        elif p+\"’s\".decode(\"utf-8\")==p2:\n",
    "                            to_del[prof].append(p2)\n",
    "                            #print p,\"=>\",p2  \n",
    "                        elif (p2.split(\" \")[0]==p):\n",
    "                            to_del[prof].append(p) #exclude p from list\n",
    "                            #print p,\"=>\",p2\n",
    "                        elif (p2.split(\" \")[-1]==p):\n",
    "                            to_del[prof].append(p)\n",
    "                            #print p,\"=>\",p2\n",
    "                        elif (len(p.split(\" \"))==2)&(p2[:len(p)]==p):\n",
    "                            #print p,\"=>\",p2\n",
    "                            #print prof\n",
    "                            to_del[prof].append(p)\n",
    "                        elif (p2[-len(p):]==p):\n",
    "                            #print p,\"=>\",p2\n",
    "                            to_del[prof].append(p)\n",
    "                        elif (p2[-len(p)-1:]==p+\"s\"):\n",
    "                            #print p,\"=>\",p2\n",
    "                            to_del[prof].append(p2)\n",
    "                            to_del[prof].append(p)\n",
    "                            to_add[prof].append(p2[:-1])\n",
    "                        elif (\"Marceau\" in p)& (\"Marcel Marceau\" in p2):\n",
    "                            to_del[prof].append(p2)\n",
    "                            to_del[prof].append(p)\n",
    "                            to_add[prof].append(\"Marcel Marceau\")\n",
    "                        elif (p in p2.split(\" \"))&(len(p2.split(\" \"))>1):\n",
    "                            #print p,\"=>\",p2\n",
    "                            to_del[prof].append(p)\n",
    "                        #else:\n",
    "                            #print p,\"=>\",p2\n",
    "    to_del[prof]=list(set(to_del[prof]))\n",
    "    to_add[prof]=list(set(to_add[prof]))\n",
    "#del\n",
    "for prof in to_del:\n",
    "    if len(to_del[prof])>0:\n",
    "        #print to_del[prof]\n",
    "        names[prof]=[n for n in names[prof] if n not in to_del[prof]]\n",
    "        names[prof]=list(set(names[prof]))\n",
    "\n",
    "\n",
    "print\"===Add\"\n",
    "#add\n",
    "for prof in to_add:\n",
    "    if len(to_add[prof])>0:\n",
    "        #print to_add[prof]\n",
    "        names[prof].extend(to_add[prof])\n",
    "        names[prof]=list(set(names[prof]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emmerich )\n",
      "0 1\n",
      "===Add\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "#parse \n",
    "\n",
    "def del_and_add_to_dict(names,names_add,names_del):\n",
    "    print \"We add:\",len(names_add),\"new instances and del \",len(names_del)#1,9\n",
    "    for prof in names_del:\n",
    "        if len(names_del[prof])>0:\n",
    "            names[prof]=[n for n in names[prof] if n not in names_del[prof]]\n",
    "            names[prof]=list(set(names[prof]))\n",
    "    for prof in names_add:\n",
    "        if len(names_add[prof])>0:\n",
    "            names[prof].extend(names_add[prof])\n",
    "            names[prof]=list(set(names[prof])) \n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "names_add=defaultdict(list)\n",
    "names_del=defaultdict(list)\n",
    "for prof in names:\n",
    "    for p in names[prof]:\n",
    "        if \"Verl\" in p:\n",
    "            if \" . Laaber Verlag\" in p:\n",
    "                new_p=p.replace(\" . Laaber Verlag\",\"\")\n",
    "                if new_p not in names[prof]:\n",
    "                    names_add[prof].append(new_p)\n",
    "            names_del[prof].append(p)\n",
    "        elif \" . \" in p:\n",
    "            new_p=p.replace(\" . \", \". \")\n",
    "            names_del[prof].append(p)\n",
    "            if new_p not in names[prof]:\n",
    "                names_add[prof].append(new_p)\n",
    "        elif (\". \" in p):\n",
    "            pass\n",
    "        elif (\".\" in p) | (\"&\" in p) |(\"/\" in p) | (\")\" in p):\n",
    "            names_del[prof].append(p)\n",
    "            print p\n",
    "        elif \"–\".decode(\"utf-8\") in p:#its not a minus  it is deffise\n",
    "            new_p=p.split(\"–\".decode(\"utf-8\"))[0].strip()\n",
    "            if (new_p not in names[prof]) & (new_p!=\"Attrice\"):\n",
    "                names_add[prof].append(new_p)\n",
    "            names_del[prof].append(p)\n",
    "        elif (p==\"Sta\")|(p==\"Erw\")|(p==\"Ed\")|(p==\"Ma\")|(p==\"Ass\")|((\"Law\"in p)&(\"urist\"in prof))\\\n",
    "        |((p==\"Jun\")&(prof==\"Arzt\"))|((p==\"Kyu\")&(prof==\"Polizeivollzugsbeamter\"))|((p==\"May\")&(prof==\"Butler\"))\\\n",
    "        |((p==\"Arx\")&(prof==\"Diakon\"))|((p==\"Wes\")&(prof==\"Unternehmensberater\"))|(p==\"Éd\".decode(\"utf-8\"))\\\n",
    "        |((p==\"Aja\")&(prof==\"Richter\"))|((p==\"Due\")&(prof==\"Wirtschaftsprüfer\".decode(\"utf-8\")))\\\n",
    "        |((p==\"Hon\")&(prof==\"Professor\")):#Law/Laws in Juist\n",
    "            names_del[prof].append(p)\n",
    "            #print p,\",\",new_p,\",\",prof\n",
    "            \n",
    "print len(names_add),len(names_del)#79,113\n",
    "names=del_and_add_to_dict(names,names_add,names_del) \n",
    "\n",
    "names_add=defaultdict(list)\n",
    "names_del=defaultdict(list)\n",
    "for prof in names:\n",
    "    for p in names[prof]:\n",
    "        if \"verlag\" in p:\n",
    "            if \". Henschelverlag\" in p:\n",
    "                new_p=p.replace(\". Henschelverlag\",\"\").strip()\n",
    "                if new_p not in names[prof]:\n",
    "                    names_add[prof].append(new_p)\n",
    "            names_del[prof].append(p)\n",
    "            \n",
    "names=del_and_add_to_dict(names,names_add,names_del) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n"
     ]
    }
   ],
   "source": [
    "print len(names)#,len(Dump)\n",
    "with open('de/wiki/all_name_entity_person_polyglot.json', 'w') as out:\n",
    "    json.dump(names, out, indent=4, sort_keys=True,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify gender by first names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (4,5,11,12,13,14,26,27,28,29,30,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,48,49,50,51,52,53,54,55,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "names_2=pd.read_csv(\"gender_names_40000.txt\",sep=\" \",header=None,index_col=False,names=range(1,84),encoding=\"utf-8\")\n",
    "names_2=names_2[names_2.columns[:6]]\n",
    "def move_value(x):\n",
    "    if x[1] in [\"1M\",\"1F\",\"?M\",\"?F\"]:\n",
    "        return x[2]\n",
    "    else:\n",
    "        return x[3]\n",
    "names_2[\"first_name\"]=names_2.apply(lambda x:  move_value(x),axis=1) #all M F in third column\n",
    "names_3=names_2[~names_2[1].isin([\"?\",\"?F\",\"?M\",\"=\"])]\n",
    "names_3 = names_3[[1,'first_name']]\n",
    "names_4 = names_3[[1,'first_name']]\n",
    "names_4[\"first_name\"]=names_4.apply(lambda x: x.first_name.replace(\"<r^>\",u'\\u0159').replace(u'\\u2030',u'\\xe4').replace(\"<l/>\",u'\\u0142').title().replace(\"è\".decode(\"utf-8\"),\"é\".decode(\"utf-8\")).\\\n",
    "                                    replace(u'\\xec',u'\\xed'),axis=1)\n",
    "#Examples wher names_4 is usefull:\n",
    "#u'Napoléon',u'Karolína',u'Bedřich',u'Czesław',u'Bartholomäus',u'Mathäus',u'Kätchen',u'Bolesław',u'Stanisław'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:12: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "with open('de/wiki/all_name_entity_person_polyglot.json', 'r') as in_f:\n",
    "    names=json.load(in_f,encoding=\"utf-8\")\n",
    "\n",
    "df_names_gender=pd.read_csv(\"name_gender_genderiz.txt\", sep=\" \",header=None,\n",
    "                            names=[\"name\",\"gender\",\"conf\",\"n\"],index_col=False,encoding=\"utf-8\")\n",
    "df_names_gender.gender=df_names_gender.gender.str.replace(\"\\[\\(u'\",\"\").str.replace(\"\\',\",\"\") \n",
    "df_names_gender.conf=df_names_gender.conf.str.replace(\"u'\",\"\").str.replace(\"\\',\",\"\").convert_objects(convert_numeric=True)\n",
    "\n",
    "df_names_gender_None=df_names_gender[df_names_gender.gender.str.contains(\"None\")]\n",
    "df_names_gender=df_names_gender[df_names_gender.gender.str.contains(\"male\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>conf</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Klaus-Peter</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Hans-Michael</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Abdulwahed</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>(Jeff)</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Schahram</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Aryya</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>Hanns-Christian</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>Maria-Eugenia</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>Khosraviyan</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>Davari</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46252</th>\n",
       "      <td>Muhammadou</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  gender  conf      n\n",
       "292        Klaus-Peter    male     0  0.0)]\n",
       "317       Hans-Michael    male     0  0.0)]\n",
       "717         Abdulwahed    male     0  0.0)]\n",
       "778             (Jeff)    male     0  0.0)]\n",
       "871           Schahram    male     0  0.0)]\n",
       "892              Aryya    male     0  0.0)]\n",
       "910    Hanns-Christian    male     0  0.0)]\n",
       "943      Maria-Eugenia  female     0  0.0)]\n",
       "3410       Khosraviyan    male     0  0.0)]\n",
       "3411            Davari    male     0  0.0)]\n",
       "46252       Muhammadou    male     0  0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(df_names_gender)#35320\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We have such examples:\n",
    "#\"K\\u00f6nig Alexander I\",\"Deutschland Egon Erwin Kisch\"\n",
    "#We look into all words in a name and store first occurency in DB\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "names_gender=defaultdict(dict)\n",
    "all_other=[]\n",
    "all_other_1=[]\n",
    "all_other_2=[]\n",
    "all_other_none=[]\n",
    "for prof in names:\n",
    "    for p in names[prof]:\n",
    "        if (p.split(\" \",1)[0] in df_names_gender.name.values):\n",
    "            gender=df_names_gender[df_names_gender.name==p.split(\" \",1)[0]].gender.values[0]\n",
    "            if gender not in names_gender[prof]:\n",
    "                names_gender[prof][gender]=[]\n",
    "            names_gender[prof][gender].append(p)   \n",
    "        elif p.split(\" \",1)[0] in names_3.first_name.values: \n",
    "            g=names_3[names_3.first_name==p.split(\" \",1)[0]][1].values[0]\n",
    "            if (g==\"F\")|(g==\"1F\"):\n",
    "                gender=\"female\"\n",
    "            else:\n",
    "                gender=\"male\"\n",
    "            if gender not in names_gender[prof]:\n",
    "                names_gender[prof][gender]=[] \n",
    "            names_gender[prof][gender].append(p)\n",
    "        elif p.split()[0] in names_4.first_name.values: \n",
    "            g=names_4[names_4.first_name==p.split(\" \",1)[0]][1].values[0]\n",
    "            if (g==\"F\")|(g==\"1F\"):\n",
    "                gender=\"female\"\n",
    "            else:\n",
    "                gender=\"male\"\n",
    "            if gender not in names_gender[prof]:\n",
    "                names_gender[prof][gender]=[] \n",
    "            names_gender[prof][gender].append(p)\n",
    "        elif ((p.split(\" \",1)[0][-1:]==\"s\") & (p.split(\" \",1)[0][:-1] in df_names_gender.name.values)&\\\n",
    "             (p not in [\"Criss Angel\",\"Mimes\",\"Tages\",\"Christians\",\"Dantes\",\"Hors\",\"Reuters\"])):\n",
    "            gender=df_names_gender[df_names_gender.name==p.split(\" \",1)[0][:-1]].gender.values[0]\n",
    "            if len(p.split())==1:\n",
    "                real_name=p.split(\" \",1)[0][:-1]\n",
    "            else:\n",
    "                first_name,last_name=p.split(\" \",1)\n",
    "                real_name=first_name[:-1]+\" \"+last_name\n",
    "            #print \"=\",p,\",\",real_name,\",\",prof\n",
    "            #check weather real_name in the list of names\n",
    "            if [i for i in names[prof] if ((p.split()[0][:-1] in i)&(i!=p)) ]:\n",
    "                pass\n",
    "            else:\n",
    "                if gender not in names_gender[prof]:\n",
    "                    names_gender[prof][gender]=[]\n",
    "                names_gender[prof][gender].append(real_name)\n",
    "        elif (len(p.split())<3)&((p.split()[0] in df_names_gender_None.name.values)):\n",
    "            all_other_none.append(p+\" , \"+prof)\n",
    "            pass\n",
    "        elif (len(p.split(\" \"))>1):\n",
    "            if (len(p.split())==2) & (not ( (p.split()[1].istitle())&(len(p.split()[1])>1))):\n",
    "                all_other.append(p+\" , \"+prof)\n",
    "                #print p\n",
    "            #if the second word starts from capital letter and more than one char\n",
    "            else:\n",
    "                found=\"\"\n",
    "                for i in p.split()[1:]:\n",
    "                    if (i.istitle())& (len(i)>1):\n",
    "                        if i in df_names_gender.name.values:\n",
    "                            #print \"!!found! in df_names_gender:\",p\n",
    "                            gender=df_names_gender[df_names_gender.name==i].gender.values[0]\n",
    "                            if gender not in names_gender[prof]:\n",
    "                                names_gender[prof][gender]=[]\n",
    "                            names_gender[prof][gender].append(p)\n",
    "                            found=p\n",
    "                            break\n",
    "                        elif i in names_3.first_name.values:\n",
    "                            #print \"!!found! in names_3:\",p\n",
    "                            g=names_3[names_3.first_name==i][1].values[0]\n",
    "                            if (g==\"F\")|(g==\"1F\"):\n",
    "                                gender=\"female\"\n",
    "                            else:\n",
    "                                gender=\"male\"\n",
    "                            if gender not in names_gender[prof]:\n",
    "                                names_gender[prof][gender]=[] \n",
    "                            names_gender[prof][gender].append(p)\n",
    "                            found=p\n",
    "                            break\n",
    "                if len(found)==0:\n",
    "                    all_other_2.append(p+\" , \"+prof)\n",
    "                    #print p+\" , \"+prof\n",
    "                        \n",
    "        else:\n",
    "            all_other_1.append(p+\" , \"+prof)\n",
    "#print all_other,all_other_1,all_other_2,all_other_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#store everything filtered from algorithm\n",
    "import pickle\n",
    "\n",
    "# obj0, obj1, obj2 are created here...\n",
    "\n",
    "# Saving the objects:\n",
    "with open('de/all_not_person_polyglot.pckl', 'w') as f:\n",
    "    pickle.dump([all_other,all_other_1,all_other_2,all_other_none], f)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open('de/all_not_person_polyglot.pckl') as f:\n",
    "    all_other,all_other_1,all_other_2,all_other_none = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add counts\n",
    "for prof in names_gender:\n",
    "    if names_gender[prof].has_key(\"male\"):\n",
    "        names_gender[prof][\"male_amount\"]=len(names_gender[prof][\"male\"])\n",
    "    if names_gender[prof].has_key(\"female\"):\n",
    "        names_gender[prof][\"female_amount\"]=len(names_gender[prof][\"female\"])\n",
    "        \n",
    "        \n",
    "with open('de/all_person_polyglot_new.json', 'w') as out:\n",
    "    json.dump(names_gender, out, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford approach==> Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Found C:\\Java\\jdk1.8.0_25\\bin\\java.exe: C:\\Java\\jdk1.8.0_25\\bin\\java.exe]\n",
      "CRFClassifier invoked on Thu Mar 03 01:54:17 CET 2016 with arguments:\n",
      "   -loadClassifier C:\\Users\\Lelka\\Downloads\\stanford-ner-2015-12-09\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz -textFile c:\\users\\lelka\\appdata\\local\\temp\\tmptwpxy4 -outputFormat slashTags\n",
      "loadClassifier=C:\\Users\\Lelka\\Downloads\\stanford-ner-2015-12-09\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz\n",
      "textFile=c:\\users\\lelka\\appdata\\local\\temp\\tmptwpxy4\n",
      "outputFormat=slashTags\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n",
      "\tat edu.stanford.nlp.io.IOUtils.<clinit>(IOUtils.java:42)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1484)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1497)\n",
      "\tat edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)\n",
      "Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n",
      "\tat java.net.URLClassLoader.findClass(Unknown Source)\n",
      "\tat java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\n",
      "\tat java.lang.ClassLoader.loadClass(Unknown Source)\n",
      "\t... 4 more\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e30786da34cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'PERSON'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 verbose=verbose)\n\u001b[1;32m---> 54\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n\u001b[0;32m     56\u001b[0m                 env_vars=('STANFORD_MODELS',), verbose=verbose)\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mbatch_tag\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'-encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# Write the actual sentences to the temporary input file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'Expected %s at %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[0m_STRING_START_RE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[uU]?[rR]?(\\\"\\\"\\\"|\\'\\'\\'|\\\"|\\')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Java command failed!"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "nltk.internals.config_java(\"C:\\\\Java\\\\jdk1.8.0_25\\\\bin\\\\java.exe\")#jre1.8.0_66\n",
    "path_to_model =\"C:\\Users\\Lelka\\Downloads\\stanford-ner-2015-12-09\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz\"\n",
    "path_to_jar = \"C:\\Users\\Lelka\\Downloads\\stanford-ner-2015-12-09\\stanford-ner-2015-12-09\\stanford-ner-3.6.0.jar\"\n",
    "st = NERTagger(path_to_model,path_to_jar)#, 'stanford-ner/stanford-ner.jar')\n",
    "#st = NERTagger(\"C:\\Users\\Lelka\\Downloads\\stanford-german-2015-10-14-models.jar\",\n",
    "#               \"C:\\Users\\Lelka\\Downloads\\stanford-ner-2015-12-09\\stanford-ner-2015-12-09\\stanford-ner.jar\")\n",
    "#download from http://nlp.stanford.edu/software/CRF-NER.html\n",
    "\n",
    "#st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "#from nltk.tag import StanfordPOSTagger\n",
    "#st = StanfordPOSTagger('english-bidirectional-distsim.tagger') \n",
    "text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON': print tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-postagger.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-postagger.jar, see:\n    <http://nlp.stanford.edu/software>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-3a12a85c70fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english-bidirectional-distsim.tagger'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordPOSTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 verbose=verbose)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    650\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[0;32m    651\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[1;32m--> 652\u001b[1;33m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    645\u001b[0m                     (name_pattern, url))\n\u001b[0;32m    646\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-postagger.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-postagger.jar, see:\n    <http://nlp.stanford.edu/software>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "st = StanfordPOSTagger('english-bidirectional-distsim.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRFClassifier invoked on Thu Mar 03 14:40:03 CET 2016 with arguments:\n",
      "   -loadClassifier C:\\Stanford\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz -textFile c:\\users\\zagovooa\\appdata\\local\\temp\\tmpmepzqe -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions \"tokenizeNLs=false\" -encoding utf8\r\n",
      "tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer\r\n",
      "tokenizerOptions=\"tokenizeNLs=false\"\r\n",
      "loadClassifier=C:\\Stanford\\stanford-ner-2015-12-09\\classifiers\\english.all.3class.distsim.crf.ser.gz\r\n",
      "encoding=utf8\r\n",
      "textFile=c:\\users\\zagovooa\\appdata\\local\\temp\\tmpmepzqe\r\n",
      "outputFormat=slashTags\r\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\r\n",
      "\tat edu.stanford.nlp.io.IOUtils.<clinit>(IOUtils.java:42)\r\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1484)\r\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1497)\r\n",
      "\tat edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)\r\n",
      "Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\r\n",
      "\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n",
      "\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n",
      "\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n",
      "\t... 4 more\r\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['C:\\\\Java\\\\jdk1.8.0_73\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'c:\\\\stanford\\\\stanford-ner-2015-12-09\\\\stanford-ner-3.6.0.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'C:\\\\Stanford\\\\stanford-ner-2015-12-09\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz', '-textFile', 'c:\\\\users\\\\zagovooa\\\\appdata\\\\local\\\\temp\\\\tmpmepzqe', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf8']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0f060de0f5c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz',\n\u001b[0;32m      5\u001b[0m                        path_to_jar=\"c:\\stanford\\stanford-ner-2015-12-09\\stanford-ner-3.6.0.jar\") \n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rami Eid is studying at Stony Brook University in NY'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\tag\\stanford.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Run the tagger and get the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[1;32m---> 89\u001b[1;33m                                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Java command failed : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Java command failed : ['C:\\\\Java\\\\jdk1.8.0_73\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'c:\\\\stanford\\\\stanford-ner-2015-12-09\\\\stanford-ner-3.6.0.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'C:\\\\Stanford\\\\stanford-ner-2015-12-09\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz', '-textFile', 'c:\\\\users\\\\zagovooa\\\\appdata\\\\local\\\\temp\\\\tmpmepzqe', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf8']"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "#nltk.internals.config_java(\"C:\\\\Java\\\\jdk1.8.0_73\\\\bin\\\\java.exe\")\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz',\n",
    "                       path_to_jar=\"c:\\stanford\\stanford-ner-2015-12-09\\stanford-ner-3.6.0.jar\") \n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Approach (too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "\n",
    "def identify_person_in_text(text):\n",
    "    person_list = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        #print sentence\n",
    "        sent1 = nltk.word_tokenize(sentence)\n",
    "        sent2 = nltk.pos_tag(sent1)\n",
    "        sent3 =  nltk.ne_chunk(sent2,binary = False)#, binary=True)\n",
    "        #print sent3\n",
    "        #sent3.draw()\n",
    "        person_list.append(re.findall(r'PERSON\\s(.*?)/',str(sent3)))\n",
    "    return person_list\n",
    "def identify_person_in_text2(text):\n",
    "    person_list = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))).subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "            person_list.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    return person_list\n",
    "\n",
    "def identify_person_in_text3(text):\n",
    "    person_list = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        #print sentence\n",
    "        sent1 = nltk.word_tokenize(sentence)\n",
    "        sent2 = nltk.pos_tag(sent1)\n",
    "        sent3 =  nltk.ne_chunk(sent2,binary = False)#, binary=True)\n",
    "        person = []\n",
    "        name = \"\"\n",
    "        for subtree in sent3.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "            for leaf in subtree.leaves():\n",
    "                person.append(leaf[0])\n",
    "                #print person\n",
    "            if len(person) > 1: #avoid grabbing lone surnames\n",
    "                for part in person:\n",
    "                    name += part + ' '\n",
    "                if name[:-1] not in person_list:\n",
    "                    person_list.append(name[:-1])\n",
    "                name = ''\n",
    "            else:\n",
    "                person_list.append(person[0])\n",
    "            person = []\n",
    "    return person_list\n",
    "\n",
    "\n",
    "#######==========Example 1\n",
    "\n",
    "import progressbar\n",
    "from itertools import groupby\n",
    "import json\n",
    "\n",
    "bar = progressbar.ProgressBar(maxval=5, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "text=u\"In Großbritannien war Gandhi und Arnim mit dem westlichen Lebensstil vertraut geworden. Ralf Lämmel war nett. Olga Zagovora kommt auch.\"\n",
    "person_nltk={}\n",
    "\n",
    "with open('de/wiki/all_data_from pages.json', 'r') as in_f:\n",
    "    Dump=json.load(in_f)\n",
    "text=Dump[\"Abfallbeauftragter\"][\"text\"]\n",
    "j=0\n",
    "bar.start()\n",
    "if True:#for article in Dump:\n",
    "    bar.update(j+1)\n",
    "    if j<=3:\n",
    "        #text=Dump[article][\"text\"]\n",
    "        person_nltk[article]=identify_person_in_text2(text)\n",
    "    #else:\n",
    "        #break\n",
    "        \n",
    "    j+=1\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.6410712819\n",
      "65.3662776135\n",
      "65.4470353946\n",
      "96.695457625\n"
     ]
    }
   ],
   "source": [
    "#============ Example 2\n",
    "import timeit\n",
    "def wrapper(func, *args, **kwargs):\n",
    "    def wrapped():\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "text1=\"I love you. Irma loves Ralf.\"\n",
    "text2=u\"In Großbritannien war Gandhi und Arnim mit dem westlichen Lebensstil vertraut geworden. Alfred Lämmel war nett. Olga Zagovora kommt auch.\"\n",
    "\n",
    "wrapped1=wrapper(identify_person_in_text,text1)\n",
    "print timeit.timeit(wrapped1, number=3)\n",
    "wrapped2=wrapper(identify_person_in_text2,text1)\n",
    "print timeit.timeit(wrapped2, number=3)\n",
    "wrapped3=wrapper(identify_person_in_text3,text1)\n",
    "print timeit.timeit(wrapped3, number=3)\n",
    "#wrapped4=wrapper(identify_person_in_text3,text2)\n",
    "#print timeit.timeit(wrapped4,number=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
